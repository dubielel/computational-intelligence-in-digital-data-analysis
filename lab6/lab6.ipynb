{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a46675f4f7dde",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T07:15:00.219831Z",
     "start_time": "2024-05-27T07:14:59.455365Z"
    }
   },
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_pettingzoo_ma_ataripy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pettingzoo.atari import flag_capture_v2"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "TORCH_DETERMINISTIC = True\n",
    "CAPTURE_VIDEO = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T07:15:00.222275Z",
     "start_time": "2024-05-27T07:15:00.220737Z"
    }
   },
   "id": "1932e6a080cd8799",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    'n_timesteps': 50_000,  # total timesteps of the experiments\n",
    "    'learning_rate': 2.5e-4,  # the learning rate of the optimizer\n",
    "    'n_envs': 16,  # the number of parallel game environments\n",
    "    'n_steps': 128,  # the number of steps to run in each environment per policy rollout\n",
    "    'anneal_lr': True,  # toggle learning rate annealing for policy and value networks\n",
    "    'gamma': 0.99,  # the discount factor gamma\n",
    "    'gae_lambda': 0.95,  # the lambda for the general advantage estimation\n",
    "    'n_minibatches': 4,  # the number of mini-batches\n",
    "    'update_epochs': 4,  # the K epochs to update the policy\n",
    "    'norm_adv': True,  # Toggles advantages normalization\n",
    "    'clip_coef': 0.1,  # the surrogate clipping coefficient\n",
    "    'clip_vloss': True,  # Toggles whether to use a clipped loss for the value function, as per the paper\n",
    "    'ent_coef': 0.01,  # coefficient of the entropy\n",
    "    'vf_coef': 0.5,  # coefficient of the value function\n",
    "    'max_grad_norm': 0.5,  # the maximum norm for the gradient clipping\n",
    "    'target_kl': None,  # the target KL divergence threshold (float)\n",
    "\n",
    "    'batch_size': 16 * 128,\n",
    "    'minibatch_size': (16 * 128) // 4\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T07:15:00.224802Z",
     "start_time": "2024-05-27T07:15:00.222806Z"
    }
   },
   "id": "1b090a9cb4f0f524",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T07:15:00.227083Z",
     "start_time": "2024-05-27T07:15:00.225295Z"
    }
   },
   "id": "54ebc4bcdc329b3a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(6, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        x = x.clone()\n",
    "        x[:, :, :, [0, 1, 2, 3]] /= 255.0\n",
    "        return self.critic(self.network(x.permute((0, 3, 1, 2))))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        x = x.clone()\n",
    "        x[:, :, :, [0, 1, 2, 3]] /= 255.0\n",
    "        hidden = self.network(x.permute((0, 3, 1, 2)))\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T07:15:00.231424Z",
     "start_time": "2024-05-27T07:15:00.228248Z"
    }
   },
   "id": "147ffec1c03da9ef",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milosz/anaconda3/envs/CI-in-DDA/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/Users/milosz/anaconda3/envs/CI-in-DDA/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/Users/milosz/anaconda3/envs/CI-in-DDA/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/var/folders/r2/5d0ljw1n2mq0j8_008pgx0dw0000gn/T/ipykernel_41770/379055736.py:52: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  next_obs = torch.Tensor(envs.reset()).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[[  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]]],\n",
      "\n",
      "\n",
      "       [[[  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]]],\n",
      "\n",
      "\n",
      "       [[[  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]]],\n",
      "\n",
      "\n",
      "       [[[  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0],\n",
      "         [  0,   0,   0,   0, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0],\n",
      "         [  0,   0,   0, 175, 255,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]],\n",
      "\n",
      "        [[  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0],\n",
      "         [  0,   0,   0, 176, 255,   0]]],\n",
      "\n",
      "\n",
      "       [[[  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255],\n",
      "         [  0,   0,   0,   0,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255],\n",
      "         [  0,   0,   0, 175,   0, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]],\n",
      "\n",
      "        [[  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         ...,\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255],\n",
      "         [  0,   0,   0, 176,   0, 255]]]], dtype=uint8), [{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 84 at dim 2 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 52\u001B[0m\n\u001B[1;32m     50\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(envs\u001B[38;5;241m.\u001B[39mreset())\n\u001B[0;32m---> 52\u001B[0m next_obs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(envs\u001B[38;5;241m.\u001B[39mreset())\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     53\u001B[0m next_done \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(HYPERPARAMETERS[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_envs\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     54\u001B[0m num_updates \u001B[38;5;241m=\u001B[39m HYPERPARAMETERS[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_timesteps\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m HYPERPARAMETERS[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mValueError\u001B[0m: expected sequence of length 84 at dim 2 (got 0)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_name = f\"flag_capture_v2__ppo_pettingzoo_ma_atari__{SEED}__{int(time.time())}\"\n",
    "\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in HYPERPARAMETERS.items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = TORCH_DETERMINISTIC\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    env = flag_capture_v2.parallel_env()\n",
    "    env = ss.max_observation_v0(env, 2)\n",
    "    env = ss.frame_skip_v0(env, 4)\n",
    "    env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)\n",
    "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.frame_stack_v1(env, 4)\n",
    "    env = ss.agent_indicator_v0(env, type_only=False)\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    envs = ss.concat_vec_envs_v1(env, HYPERPARAMETERS['n_envs'] // 2, num_cpus=0, base_class=\"gymnasium\")\n",
    "    envs.single_observation_space = envs.observation_space\n",
    "    envs.single_action_space = envs.action_space\n",
    "    envs.is_vector_env = True\n",
    "    envs = gym.wrappers.RecordEpisodeStatistics(envs)\n",
    "    if CAPTURE_VIDEO:\n",
    "        envs = gym.wrappers.RecordVideo(envs, f\"videos/{run_name}\")\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "    agent = Agent(envs).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=HYPERPARAMETERS['learning_rate'], eps=1e-5)\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = torch.zeros((HYPERPARAMETERS['n_steps'], HYPERPARAMETERS['n_envs']) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((HYPERPARAMETERS['n_steps'], HYPERPARAMETERS['n_envs']) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((HYPERPARAMETERS['n_steps'], HYPERPARAMETERS['n_envs'])).to(device)\n",
    "    rewards = torch.zeros((HYPERPARAMETERS['n_steps'], HYPERPARAMETERS['n_envs'])).to(device)\n",
    "    dones = torch.zeros((HYPERPARAMETERS['n_steps'], HYPERPARAMETERS['n_envs'])).to(device)\n",
    "    values = torch.zeros((HYPERPARAMETERS['n_steps'], HYPERPARAMETERS['n_envs'])).to(device)\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.zeros(HYPERPARAMETERS['n_envs']).to(device)\n",
    "    num_updates = HYPERPARAMETERS['n_timesteps'] // HYPERPARAMETERS['batch_size']\n",
    "\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if HYPERPARAMETERS['anneal_lr']:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * HYPERPARAMETERS['learning_rate']\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, HYPERPARAMETERS['n_steps']):\n",
    "            global_step += 1 * HYPERPARAMETERS['n_envs']\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "            for idx, item in enumerate(info):\n",
    "                player_idx = idx % 2\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(f\"global_step={global_step}, {player_idx}-episodic_return={item['episode']['r']}\")\n",
    "                    writer.add_scalar(f\"charts/episodic_return-player{player_idx}\", item[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(f\"charts/episodic_length-player{player_idx}\", item[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(HYPERPARAMETERS['n_steps'])):\n",
    "                if t == HYPERPARAMETERS['n_steps'] - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + HYPERPARAMETERS['gamma'] * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + HYPERPARAMETERS['gamma'] * HYPERPARAMETERS['gae_lambda'] * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(HYPERPARAMETERS['batch_size'])\n",
    "        clipfracs = []\n",
    "        for epoch in range(HYPERPARAMETERS['update_epochs']):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, HYPERPARAMETERS['batch_size'], HYPERPARAMETERS['minibatch_size']):\n",
    "                end = start + HYPERPARAMETERS['minibatch_size']\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > HYPERPARAMETERS['clip_coef']).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if HYPERPARAMETERS['norm_adv']:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - HYPERPARAMETERS['clip_coef'], 1 + HYPERPARAMETERS['clip_coef'])\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if HYPERPARAMETERS['clip_vloss']:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -HYPERPARAMETERS['clip_coef'],\n",
    "                        HYPERPARAMETERS['clip_coef'],\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - HYPERPARAMETERS['ent_coef'] * entropy_loss + v_loss * HYPERPARAMETERS['vf_coef']\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), HYPERPARAMETERS['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "\n",
    "            if HYPERPARAMETERS['target_kl'] is not None:\n",
    "                if approx_kl > HYPERPARAMETERS['target_kl']:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T07:15:01.797847Z",
     "start_time": "2024-05-27T07:15:00.232060Z"
    }
   },
   "id": "initial_id",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
