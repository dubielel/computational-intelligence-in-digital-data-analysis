{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprawozdanie 4 - Własne Środowisko\n",
    "Autorzy: Kacper Cienkosz, Miłosz Dubiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gra \"Zgadnij liczbę\"\n",
    "\n",
    "### Opis:\n",
    "\n",
    "Gra dwóch graczy: gracz i komputer.\n",
    "\n",
    "Gracz wybiera przedział liczb, np. od 1 do 100.\n",
    "\n",
    "Komputer losuje liczbę z tego przedziału, której gracz musi się domyślić.\n",
    "\n",
    "Gracz próbuje zgadnąć liczbę, a komputer informuje, czy podana liczba jest za duża, za mała, czy trafiona.\n",
    "\n",
    "### Implementacja:\n",
    "\n",
    "1. Inicjalizacja gry:\n",
    "    * Gracz wybiera przedział liczb.\n",
    "    * Komputer losuje liczbę z tego przedziału.\n",
    "\n",
    "2. Rozpoczęcie rundy:\n",
    "    * Gracz podaje swoją propozycję liczby.\n",
    "    * Komputer sprawdza, czy podana liczba jest prawidłowa:\n",
    "        - Jeśli liczba jest trafiona, komputer informuje o tym gracza, a gra kończy się.\n",
    "        - Jeśli liczba jest za duża lub za mała, komputer daje odpowiednią wskazówkę.\n",
    "\n",
    "3. Powtórzenie rundy, aż gracz zgadnie liczbę.\n",
    "\n",
    "4. Podanie wyniku, czy gracz zgadł liczbę.\n",
    "\n",
    "Ta gra wymaga od gracza logicznego myślenia i podejmowania decyzji na podstawie informacji zwrotnych, co czyni ją ciekawą do eksperymentowania z różnymi strategiami uczenia ze wzmocnieniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "from tqdm import tqdm\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3964068844.py, line 86)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    match self.algorithm:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "\n",
    "# %%\n",
    "# Declaration and Initialization\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Our custom environment will inherit from the abstract class\n",
    "# ``gymnasium.Env``. You shouldn’t forget to add the ``metadata``\n",
    "# attribute to your class. There, you should specify the render-modes that\n",
    "# are supported by your environment (e.g. ``\"human\"``, ``\"rgb_array\"``,\n",
    "# ``\"ansi\"``) and the framerate at which your environment should be\n",
    "# rendered. Every environment should support ``None`` as render-mode; you\n",
    "# don’t need to add it in the metadata. In ``GridWorldEnv``, we will\n",
    "# support the modes “rgb_array” and “human” and render at 4 FPS.\n",
    "#\n",
    "# The ``__init__`` method of our environment will accept the integer\n",
    "# ``size``, that determines the size of the square grid. We will set up\n",
    "# some variables for rendering and define ``self.observation_space`` and\n",
    "# ``self.action_space``. In our case, observations should provide\n",
    "# information about the location of the agent and target on the\n",
    "# 2-dimensional grid. We will choose to represent observations in the form\n",
    "# of dictionaries with keys ``\"agent\"`` and ``\"target\"``. An observation\n",
    "# may look like ``{\"agent\": array([1, 0]), \"target\": array([0, 3])}``.\n",
    "# Since we have 4 actions in our environment (“right”, “up”, “left”,\n",
    "# “down”), we will use ``Discrete(4)`` as an action space. Here is the\n",
    "# declaration of ``GridWorldEnv`` and the implementation of ``__init__``:\n",
    "\n",
    "\n",
    "class GuessNumberEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": ['human'], \"render_fps\": 1}\n",
    "\n",
    "    def __init__(self, algorithm, render_mode=None, min_number=1, max_number=100):\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "        self._agent_number = None\n",
    "        self._target_number = None\n",
    "\n",
    "        self.min_number = min_number  # The number which is the lower threshold for the numbers range\n",
    "        self.max_number = max_number  # The number which is the upper threshold for the numbers range \n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's numbers.\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Discrete(max_number - min_number + 1),\n",
    "                \"target\": spaces.Discrete(max_number - min_number + 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # The action in this env is a number from the range [min_number, max_number]\n",
    "        self.action_space = spaces.Discrete(max_number - min_number + 1)\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    # %%\n",
    "    # Constructing Observations From Environment States\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #\n",
    "    # Since we will need to compute observations both in ``reset`` and\n",
    "    # ``step``, it is often convenient to have a (private) method ``_get_obs``\n",
    "    # that translates the environment’s state into an observation. However,\n",
    "    # this is not mandatory and you may as well compute observations in\n",
    "    # ``reset`` and ``step`` separately:\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_number, \"target\": self._target_number}\n",
    "\n",
    "    # %%\n",
    "    # We can also implement a similar method for the auxiliary information\n",
    "    # that is returned by ``step`` and ``reset``. In our case, we would like\n",
    "    # to provide the manhattan distance between the agent and the target:\n",
    "\n",
    "    def _get_info(self):\n",
    "        diff = self._target_number - self._agent_number\n",
    "        match self.algorithm:\n",
    "            case \"QLearning\":\n",
    "                \"\"\"\n",
    "                When `self._target_number` is greater than chosen return 1\n",
    "                When `self._target_number` is less than chosen return -1\n",
    "                When `self._target_number` is equal to chosen return 0\n",
    "                \"\"\"\n",
    "                return {'distance': np.sign(diff)}\n",
    "            case \"SARSA\":\n",
    "                \"\"\"\n",
    "                When `self._target_number` is less than chosen return -2\n",
    "                When `self._target_number` is greater than chosen return -1\n",
    "                When `self._target_number` is equal to chosen return 0\n",
    "                \"\"\"\n",
    "                if diff < 0:\n",
    "                    return {'distance': -2, 'message': f'Chosen number ({self._agent_number}) is too big'}\n",
    "                elif diff > 0:\n",
    "                    return {'distance': -1, 'message': f'Chosen number ({self._agent_number}) is too small'}\n",
    "                else:\n",
    "                    return {'distance': 0, 'message': f'Correctly guessed number ({self._agent_number})!'}\n",
    "\n",
    "    # %%\n",
    "    # Oftentimes, info will also contain some data that is only available\n",
    "    # inside the ``step`` method (e.g. individual reward terms). In that case,\n",
    "    # we would have to update the dictionary that is returned by ``_get_info``\n",
    "    # in ``step``.\n",
    "\n",
    "    # %%\n",
    "    # Reset\n",
    "    # ~~~~~\n",
    "    #\n",
    "    # The ``reset`` method will be called to initiate a new episode. You may\n",
    "    # assume that the ``step`` method will not be called before ``reset`` has\n",
    "    # been called. Moreover, ``reset`` should be called whenever a done signal\n",
    "    # has been issued. Users may pass the ``seed`` keyword to ``reset`` to\n",
    "    # initialize any random number generator that is used by the environment\n",
    "    # to a deterministic state. It is recommended to use the random number\n",
    "    # generator ``self.np_random`` that is provided by the environment’s base\n",
    "    # class, ``gymnasium.Env``. If you only use this RNG, you do not need to\n",
    "    # worry much about seeding, *but you need to remember to call\n",
    "    # ``super().reset(seed=seed)``* to make sure that ``gymnasium.Env``\n",
    "    # correctly seeds the RNG. Once this is done, we can randomly set the\n",
    "    # state of our environment. In our case, we randomly choose the agent’s\n",
    "    # location and the random sample target positions, until it does not\n",
    "    # coincide with the agent’s position.\n",
    "    #\n",
    "    # The ``reset`` method should return a tuple of the initial observation\n",
    "    # and some auxiliary information. We can use the methods ``_get_obs`` and\n",
    "    # ``_get_info`` that we implemented earlier for that:\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's number uniformly at random\n",
    "        self._agent_number = self.np_random.integers(self.min_number, self.max_number, dtype=int)\n",
    "\n",
    "        # We will sample the target's number randomly until it does not coincide with the agent's number\n",
    "        self._target_number = self._agent_number\n",
    "        while self._target_number == self._agent_number:\n",
    "            self._target_number = self.np_random.integers(self.min_number, self.max_number, dtype=int)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    # %%\n",
    "    # Step\n",
    "    # ~~~~\n",
    "    #\n",
    "    # The ``step`` method usually contains most of the logic of your\n",
    "    # environment. It accepts an ``action``, computes the state of the\n",
    "    # environment after applying that action and returns the 5-tuple\n",
    "    # ``(observation, reward, terminated, truncated, info)``. See\n",
    "    # :meth:`gymnasium.Env.step`. Once the new state of the environment has\n",
    "    # been computed, we can check whether it is a terminal state and we set\n",
    "    # ``done`` accordingly. Since we are using sparse binary rewards in\n",
    "    # ``GridWorldEnv``, computing ``reward`` is trivial once we know\n",
    "    # ``done``.To gather ``observation`` and ``info``, we can again make\n",
    "    # use of ``_get_obs`` and ``_get_info``:\n",
    "\n",
    "    def step(self, action):\n",
    "        # An episode is done if the agent has reached the target\n",
    "        self._agent_number = action\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        reward = info[\"distance\"]\n",
    "        terminated = not info[\"distance\"]\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    # %%\n",
    "    # Rendering\n",
    "    # ~~~~~~~~~\n",
    "\n",
    "    def render(self):\n",
    "        # if self.render_mode == \"rgb_array\":\n",
    "        #     return self._render_frame()\n",
    "\n",
    "        match self.render_mode:\n",
    "            case \"human\":\n",
    "                print(f\"Agent number: {self._agent_number}, Target number: {self._target_number}, Reward: {self._get_info()['distance']}\")\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id=\"envs/GuessNumberEnv-v0\",\n",
    "    entry_point=\"envs:GuessNumberEnv\",\n",
    "    max_episode_steps=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opisy algorytmów użytych do uczenia ze wzmocnieniem\n",
    "\n",
    "W naszych eksperymentach wykorzystaliśmy dwa algorytmy QLearning i SARSA.\n",
    "\n",
    "### Opis działania QLearning\n",
    "\n",
    "Część teoretyczna działania algorytmu QLearning została przez nas dokładnie opisana w ramach poprzedniego laboratorium, zatem teraz przedstawimy tylko opis działania w naszej implementacji. Przy każdym zgadywaniu miał $\\epsilon$ szans na wybranie losowej liczby i $1 - \\epsilon$ na wybranie wartości z QTable. To podejście niestety nie jest dobre z dwóch powodów. Pierwszym powodem jest fakt, że QTable powinno być różne dla każdej możliwej liczby. Dla poprawnego działania algorytmu konieczne byłoby znanie liczby przez agenta, co w oczywisty sposób nie ma sensu. Drugi powód związany jest z innym podejściem do uczenia. Możnaby spróbować za pomoczą wartości w QTable ograniczać przedziały, w których może znaleźć się liczba. Jest to jednak przerost formy nad treścią, ponieważ dla każdej wylosowanej liczby przeprowadzony musiałby być osobny proces uczenia, który de facto sprowadziłby się do implementacji bin search, jednak dużo mniej efektywnej i niepotrzebnie skomplikowanej. Ostatecznie udawało się zakończyć działanie algorytmu po losowym wyborze liczby.\n",
    "\n",
    "### Opis działania SARSA\n",
    "\n",
    "Algorytm SARSA uczy się według wzoru:\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow (1 - \\alpha)Q(s_t, a_t) + \\alpha \\cdot [r_{t+1} + \\gamma \\cdot Q(s_{t+1}, a_{t+1})],$$\n",
    "\n",
    "gdzie:\n",
    "* $Q(s_t,a_t)$: aktualna ocena funkcji wartości akcji (Q-funkcji) dla stanu $s_t$ i akcji \n",
    "$a_t$. Oznacza oczekiwaną sumę nagród uzyskanych poprzez wykonanie akcji $a_t$ w stanie $s_t$.\n",
    "* $\\alpha$: współczynnik uczenia.\n",
    "* $r_{t+1}$: natychmiastowa nagroda.\n",
    "* $\\gamma$: współczynnik zmniejszający wagę przyszłych nagród.\n",
    "* $Q(s_{t+1}, a_{t+1})$: ocena wartości dla przyszłego stanu po przyszłej akcji wybranej według aktualnej polityki.\n",
    "\n",
    "Algorytm SARSA jest wariacją na temat algorytmu QLearning. Główną różnicą jest podejście *off policy* w QLearning i *on policy* w SARSA. W pierwszej polityce agent wyciąga ocenę akcji z polityki, która nie jest aktualnie w użyciu. W drugim podejściu przeciwnie, agent ocenia wartość akcji na podstawie aktualnej polityki.\n",
    "\n",
    "Źródło: [Artykuł na GeeksForGeeks](https://www.geeksforgeeks.org/sarsa-reinforcement-learning/).\n",
    "\n",
    "W naszym przypadku uczenie algorytmem SARSA również okazało się nieskuteczne. Algorytm ostatecznie zgadywał liczbę ale działał losowo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperymenty QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            learning_rate=0.1,\n",
    "            discount_factor=0.99,\n",
    "            exploration_rate=1.0,\n",
    "            min_exploration_rate=0.01,\n",
    "            exploration_decay_rate=0.99\n",
    "    ):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.q_table = np.zeros((action_space.n,))\n",
    "\n",
    "    def choose_action(self):\n",
    "        if uniform(0, 1) < self.exploration_rate:\n",
    "            return self.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            return np.argmax(self.q_table)  # Exploit learned values\n",
    "\n",
    "    def update_q_table(self, action, reward):\n",
    "        old_q_value = self.q_table[action]\n",
    "        next_max = np.max(self.q_table)\n",
    "        new_q_value = (1 - self.learning_rate) * old_q_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
    "        self.q_table[action] = new_q_value\n",
    "\n",
    "    def decay_exploration_rate(self):\n",
    "        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mq_learning_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QLearningAgent\n\u001b[0;32m----> 6\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menvs/GuessNumberEnv-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQLearning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m], env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py:756\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 756\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    759\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py:545\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 545\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Studia/Semestr6/InteligencjaObliczeniowa/computational-intelligence-in-digital-data-analysis/lab4/envs/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguess_number_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GuessNumberEnv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register\n\u001b[1;32m      4\u001b[0m register(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvs/GuessNumberEnv-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvs:GuessNumberEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     max_episode_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab4'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"envs/GuessNumberEnv-v0\", algorithm=\"QLearning\", render_mode=\"human\")\n",
    "agent = QLearningAgent(env.observation_space['agent'], env.action_space)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 1000\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action()\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        agent.update_q_table(action, reward)\n",
    "        observation = next_observation\n",
    "    agent.decay_exploration_rate()\n",
    "\n",
    "# Test the trained agent\n",
    "observation = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.choose_action()\n",
    "    next_observation, reward, done, _, _ = env.step(action)\n",
    "    observation = next_observation\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperymenty SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SARSAAgent:\n",
    "    def __init__(self, observation_space, action_space, learning_rate=0.1, discount_factor=0.99, epsilon=0.1):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((observation_space[\"agent\"].n, action_space.n))\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_space.n)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[observation[\"agent\"]])\n",
    "\n",
    "    def update_q_table(self, observation, action, reward, next_observation, next_action):\n",
    "        current_q_value = self.q_table[observation[\"agent\"], action]\n",
    "        next_q_value = self.q_table[next_observation[\"agent\"], next_action]\n",
    "        td_target = reward + self.discount_factor * next_q_value\n",
    "        td_error = td_target - current_q_value\n",
    "\n",
    "        if reward == -1:\n",
    "            # chosen number is too small, so update every possible [state, action] smaller than chosen number\n",
    "            # so that we can avoid choosing smaller numbers than currently chosen one\n",
    "            self.q_table[: observation[\"agent\"] + 1, : action + 1] += self.learning_rate * td_error\n",
    "            self.q_table[:, : action + 1] += self.learning_rate * td_error\n",
    "            self.q_table[: observation[\"agent\"] + 1, :] += self.learning_rate * td_error\n",
    "            return\n",
    "        if reward == -2:\n",
    "            # chosen number is too big\n",
    "            self.q_table[observation[\"agent\"]:, action:] += self.learning_rate * td_error\n",
    "            self.q_table[:, action:] += self.learning_rate * td_error\n",
    "            self.q_table[observation[\"agent\"]:, :] += self.learning_rate * td_error\n",
    "\n",
    "            return\n",
    "\n",
    "        self.q_table[observation[\"agent\"], action] += self.learning_rate * td_error\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        for i in tqdm(range(episodes)):\n",
    "            observation = env.reset()\n",
    "            action = observation['agent']\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_observation, reward, done, _, info = env.step(action)\n",
    "                # if i in [0, 50, 99]:\n",
    "                #     print(f'{i} {info[\"message\"]}')\n",
    "                next_action = self.choose_action(next_observation)\n",
    "                self.update_q_table(observation, action, reward, next_observation, next_action)\n",
    "                observation = next_observation\n",
    "                action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarsa_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SARSAAgent\n\u001b[1;32m      5\u001b[0m env_sarsa \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvs/GuessNumberEnv-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSARSA\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m agent \u001b[38;5;241m=\u001b[39m SARSAAgent(env_sarsa\u001b[38;5;241m.\u001b[39mobservation_space, env_sarsa\u001b[38;5;241m.\u001b[39maction_space)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab4'"
     ]
    }
   ],
   "source": [
    "env_sarsa = gym.make(\"envs/GuessNumberEnv-v0\", algorithm=\"SARSA\", render_mode=\"human\")\n",
    "agent = SARSAAgent(env_sarsa.observation_space, env_sarsa.action_space)\n",
    "\n",
    "# Train the agent\n",
    "agent.train(env_sarsa, episodes=1000)\n",
    "\n",
    "# Test the agent\n",
    "observation = env_sarsa.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, _, _ = env_sarsa.step(action)\n",
    "    env_sarsa.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "Gra Zgadnij Liczbę nie jest grą dobrą do stosowania uczenia ze wzmocnieniem. Nauka algorytmów sprowadza się do implementacji algorytmu bin search, który jest niepotrzebnie skomplikowany. Kolejnym problemem jest różnorodność środowiska. Agent w idealnej sytuacji powinien się nauczyć akcji do zgadywania konkretnej liczby, jednak wymaga to znania liczby a priori, czyli sprawia, że uczenie nie ma sensu. Naszym innym pomysłem byłoby uzależnienie nagrody od tego jak daleko jest liczba, którą zgadł agent od oczekiwanej. To podejście również jest problematyczne, ponieważ zdradza zbyt dużo informacji agentowi na starcie i sprowadza się do poznania liczby na samym początku. Uczenie ze wzmocnieniem działa dobrze w grach takich jak blackjack, gdzie możemy stworzyć odpowiednią politykę. W grze Zgadnij Liczbę nie da się określić polityki, zaś agent musiałby się nauczyć algorytmu bin search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
